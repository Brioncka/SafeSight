<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Safesight – Technical Approach & Results</title>
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
<header class="site-header">
  <div class="container header-inner">
    <div class="brand">
      <img src="assets/images/safesight-logo.png"
           alt="Safesight logo"
           class="logo" />

      <div>
        <h1>Safesight</h1>
        <p class="tagline">Preventing Accidents with AI-Powered Wildlife Detection</p>
      </div>
    </div>

    <nav class="nav">
      <a href="index.html" class="nav-link active">Home</a>
      <a href="approach.html" class="nav-link">Approach</a>
    </nav>
  </div>
</header>


  <main>
    <section class="section hero-sm">
      <div class="container">
        <h2>Technical Overview</h2>
        <p>
          Safesight is an image classification project that uses <strong>AlexNet</strong> to determine whether
          wildlife (specifically deer) is present on a driver’s route. The goal is to explore how deep learning
          can help reduce wildlife–vehicle collisions by warning drivers before a potential impact.
        </p>
        <p>
          To understand the impact of transfer learning, I compared three types of experiments:
          <strong>(1) baseline AlexNet</strong>, <strong>(2) untrained AlexNet</strong> with random initialization,
          and <strong>(3) pretrained AlexNet</strong> fine-tuned on my deer vs nodeer dataset.
        </p>
      </div>
    </section>

    <section class="section section-alt">
      <div class="container">
        <h2>Dataset</h2>

        <div class="card-grid two-col">
          <div class="card">
            <h3>Data Collection</h3>
            <ul>
              <li>Total images collected: ~<strong>320</strong> Google Images results.</li>
              <li>Final images used after loading: <strong>307</strong> (192 train, 115 validation).</li>
              <li>Classes: <strong>deer</strong>, <strong>nodeer</strong>.</li>
              <li>
                Goal: simulate dashcam-style views where a driver might encounter wildlife on or near the road.
              </li>
            </ul>
          </div>

          <div class="card">
            <h3>Split Strategy &amp; Organization</h3>
            <ul>
              <li>Training set: <strong>192</strong> images (≈ 62%).</li>
              <li>Validation set: <strong>115</strong> images (≈ 38%).</li>
              <li>Folder structure:</li>
              <ul>
                <li><code>train/deer</code> and <code>train/nodeer</code></li>
                <li><code>valid/deer</code> and <code>valid/nodeer</code></li>
              </ul>
              <li>
                Simple data augmentations such as random flips and crops help imitate small viewpoint changes a
                driver might experience.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Model: AlexNet for Binary Wildlife Detection</h2>

        <div class="card-grid two-col">
          <div class="card">
            <h3>Pretrained Backbone</h3>
            <p>
              Safesight uses the classic <strong>AlexNet</strong> architecture from PyTorch’s model zoo. The
              version pretrained on ImageNet is used as a general feature extractor that is already good at
              detecting edges, textures, and object parts.
            </p>
            <ul>
              <li>
                The early convolutional layers are reused to capture low-level and mid-level visual features
                relevant for many object categories, including deer.
              </li>
              <li>
                This lets the model start from a strong visual prior instead of learning everything from scratch
                on a small dataset.
              </li>
            </ul>
          </div>

          <div class="card">
            <h3>Classifier Modification</h3>
            <p>
              The original AlexNet classifier outputs 1000 ImageNet classes. For Safesight, I replaced the final
              classifier head to adapt the model to the two-class deer vs nodeer problem:
            </p>
            <ul>
              <li>A final fully connected layer with <strong>2 output units</strong> (deer, nodeer).</li>
              <li>Training with a standard <strong>cross-entropy loss</strong> for binary classification.</li>
              <li>
                This simple change allows the model to leverage learned ImageNet features while focusing on the
                wildlife detection task.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section class="section section-alt">
      <div class="container">
        <h2>Training Setup</h2>
        <div class="card-grid two-col">
          <div class="card">
            <h3>Hyperparameters</h3>
            <ul>
              <li>Epochs: <strong>30</strong> for all runs.</li>
              <li>Batch size: <strong>small-batch</strong> training suitable for 192 training images.</li>
              <li>Optimizer: standard gradient-based optimizer (e.g., <strong>SGD</strong> or <strong>Adam</strong>).</li>
              <li>Learning rate: small enough for stable fine-tuning of the pretrained backbone.</li>
              <li>Loss: <strong>Cross-entropy</strong> for binary classification.</li>
            </ul>
          </div>

          <div class="card">
            <h3>Experiment Tracking (Weights &amp; Biases)</h3>
            <p>
              All experiments were tracked with <strong>Weights &amp; Biases (W&amp;B)</strong>. For each run, I
              logged:
            </p>
            <ul>
              <li>Training and validation loss.</li>
              <li>Training and validation accuracy.</li>
              <li>Summary statistics such as <code>best_validation_accuracy</code>.</li>
            </ul>
            <p>
              This made it easy to compare baseline, untrained, and pretrained AlexNet runs and clearly see the
              benefit of transfer learning.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Results</h2>

        <div class="card-grid two-col">
          <div class="card">
            <h3>Quantitative Performance</h3>
            <ul>
              <li>
                <strong>Best pretrained run:</strong> <code>pretrained_run_011</code> with
                <code>best_validation_accuracy = 0.9913</code> (≈ 99.1%).
              </li>
              <li>
                Best pretrained final metrics:
                <ul>
                  <li>Training accuracy ≈ <strong>1.0</strong></li>
                  <li>Training loss ≈ <strong>2 × 10⁻⁵</strong></li>
                  <li>Validation accuracy ≈ <strong>0.974</strong></li>
                  <li>Validation loss ≈ <strong>0.098</strong></li>
                </ul>
              </li>
              <li>
                Typical pretrained runs: validation accuracy in the range
                <strong>0.97–0.98</strong>.
              </li>
              <li>
                Untrained (random init) AlexNet: best runs reached ≈ <strong>0.90</strong> validation accuracy,
                with many runs in the <strong>0.85–0.89</strong> range.
              </li>
              <li>
                Overall, fine-tuning a pretrained AlexNet gives roughly a
                <strong>+7–10 percentage point boost</strong> in validation accuracy compared to training AlexNet
                from scratch.
              </li>
            </ul>
          </div>

          <div class="card">
            <h3>Interpreting the Curves</h3>
            <p>
              Looking at the loss and accuracy curves for pretrained vs untrained runs:
            </p>
            <ul>
              <li>
                Pretrained runs converge faster, with validation loss dropping quickly and stabilizing at a low
                value.
              </li>
              <li>
                Validation accuracy for pretrained runs climbs rapidly to ≈ 97–99%, while untrained runs improve
                more slowly and plateau at lower accuracy.
              </li>
              <li>
                This behavior is consistent with the idea that pretrained features from ImageNet transfer well to
                the wildlife detection task, especially on a small dataset.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section class="section section-alt">
      <div class="container">
        <h2>Takeaways & Future Work</h2>
        <div class="card-grid two-col">
          <div class="card">
            <h3>Key Takeaways</h3>
            <ul>
              <li>
                Transfer learning with AlexNet provides a strong performance boost on a small, custom deer vs
                nodeer dataset.
              </li>
              <li>
                Even with limited data, careful organization, basic augmentation, and good experiment tracking
                can produce a reliable model.
              </li>
              <li>
                Tools like W&amp;B make it easy to compare many runs and avoid guessing which model is “best”.
              </li>
            </ul>
          </div>

          <div class="card">
            <h3>Future Work</h3>
            <ul>
              <li>Collect more real dashcam footage in diverse weather and lighting conditions.</li>
              <li>
                Extend the label space from binary deer/nodeer to multiple animal categories and “no animal”.
              </li>
              <li>
                Explore smaller, faster architectures for real-time deployment in vehicles or mobile devices.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>© Safesight – AlexNet Wildlife Detection Project</p>
    </div>
  </footer>
</body>
</html>

