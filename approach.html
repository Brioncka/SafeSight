<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Safesight – Technical Approach & Results</title>
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
<header class="site-header">
  <div class="container header-inner">
    <div class="brand">
      <img src="assets/images/safesight-logo.png"
           alt="Safesight logo"
           class="logo" />

      <div>
        <h1>Safesight</h1>
        <p class="tagline">Preventing Accidents with AI-Powered Wildlife Detection</p>
      </div>
    </div>

    <nav class="nav">
      <a href="index.html" class="nav-link active">Home</a>
      <a href="approach.html" class="nav-link">Approach</a>
    </nav>
  </div>
</header>


  <main>
    <section class="section hero-sm">
      <div class="container">
        <h2>Technical Overview</h2>
<div class="hero-image-card" style="margin-top: 16px; max-width: 420px;">
  <img
    src="assets/images/safesight-banner.png"
    alt="Safesight AI-powered wildlife detection banner"
  />
</div>

        <p>
          Safesight is an image classification project that uses <strong>AlexNet</strong> to determine whether
          wildlife (specifically deer) is present on a driver’s route. The goal is to explore how deep learning
          can help reduce wildlife–vehicle collisions by warning drivers before a potential impact.
        </p>
        <p>
          To understand the impact of transfer learning, I compared three types of experiments:
          <strong>(1) baseline AlexNet</strong>, <strong>(2) untrained AlexNet</strong> with random initialization,
          and <strong>(3) pretrained AlexNet</strong> fine-tuned on my deer vs nodeer dataset.
        </p>
      </div>
    </section>

    <section class="section section-alt">
      <div class="container">
        <h2>Dataset</h2>

        <div class="card-grid two-col">
          <div class="card">
            <h3>Data Collection</h3>
            <ul>
              <li>Total images collected: ~<strong>320</strong> Google Images results.</li>
              <li>Final images used after loading: <strong>307</strong> (192 train, 115 validation).</li>
              <li>Classes: <strong>deer</strong>, <strong>nodeer</strong>.</li>
              <li>
                Goal: simulate dashcam-style views where a driver might encounter wildlife on or near the road.
              </li>
            </ul>
          </div>

          <div class="card">
            <h3>Split Strategy &amp; Organization</h3>
            <ul>
              <li>Training set: <strong>192</strong> images (≈ 62%).</li>
              <li>Validation set: <strong>115</strong> images (≈ 38%).</li>
              <li>Folder structure:</li>
              <ul>
                <li><code>train/deer</code> and <code>train/nodeer</code></li>
                <li><code>valid/deer</code> and <code>valid/nodeer</code></li>
              </ul>
              <li>
                Simple data augmentations such as random flips and crops help imitate small viewpoint changes a
                driver might experience.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Model: AlexNet for Binary Wildlife Detection</h2>

        <div class="card-grid two-col">
          <div class="card">
            <h3>Pretrained Backbone</h3>
            <p>
              Safesight uses the classic <strong>AlexNet</strong> architecture from PyTorch’s model zoo. The
              version pretrained on ImageNet is used as a general feature extractor that is already good at
              detecting edges, textures, and object parts.
            </p>
            <ul>
              <li>
                The early convolutional layers are reused to capture low-level and mid-level visual features
                relevant for many object categories, including deer.
              </li>
              <li>
                This lets the model start from a strong visual prior instead of learning everything from scratch
                on a small dataset.
              </li>
            </ul>
          </div>

          <div class="card">
            <h3>Classifier Modification</h3>
            <p>
              The original AlexNet classifier outputs 1000 ImageNet classes. For Safesight, I replaced the final
              classifier head to adapt the model to the two-class deer vs nodeer problem:
            </p>
            <ul>
              <li>A final fully connected layer with <strong>2 output units</strong> (deer, nodeer).</li>
              <li>Training with a standard <strong>cross-entropy loss</strong> for binary classification.</li>
              <li>
                This simple change allows the model to leverage learned ImageNet features while focusing on the
                wildlife detection task.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section class="section section-alt">
      <div class="container">
        <h2>Training Setup</h2>
        <div class="card-grid two-col">
          <div class="card">
            <h3>Hyperparameters</h3>
            <ul>
              <li>Epochs: <strong>30</strong> for all runs.</li>
              <li>Batch size: <strong>small-batch</strong> training suitable for 192 training images.</li>
              <li>Optimizer: standard gradient-based optimizer (e.g., <strong>SGD</strong> or <strong>Adam</strong>).</li>
              <li>Learning rate: small enough for stable fine-tuning of the pretrained backbone.</li>
              <li>Loss: <strong>Cross-entropy</strong> for binary classification.</li>
            </ul>
          </div>

          <div class="card">
            <h3>Experiment Tracking (Weights &amp; Biases)</h3>
            <p>
              All experiments were tracked with <strong>Weights &amp; Biases (W&amp;B)</strong>. For each run, I
              logged:
            </p>
            <ul>
              <li>Training and validation loss.</li>
              <li>Training and validation accuracy.</li>
              <li>Summary statistics such as <code>best_validation_accuracy</code>.</li>
            </ul>
            <p>
              This made it easy to compare baseline, untrained, and pretrained AlexNet runs and clearly see the
              benefit of transfer learning.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
  <div class="container">
    <h2>Results & Experiments</h2>

    <div class="card-grid two-col">
      <div class="card">
        <h3>Overall Performance</h3>
        <p>
          I ran six main experiments on the Safesight deer vs nodeer dataset using AlexNet and ResNet18.
          Each experiment logged training and validation loss and accuracy to Weights &amp; Biases (W&amp;B),
          making it easy to compare hyperparameters and architectures.
        </p>
        <ul>
          <li>
            <strong>Best AlexNet (pretrained):</strong> validation accuracy up to
            <strong>0.9826 (≈ 98.3%)</strong> with batch size 16, learning rate 0.0001, and data augmentation.
          </li>
          <li>
            <strong>Typical strong AlexNet runs:</strong> validation accuracy in the
            <strong>0.96–0.98</strong> range for batch sizes 32 and 64 with learning rate 0.0001.
          </li>
          <li>
            <strong>AlexNet (untrained):</strong> training from scratch with random initialization reached
            only about <strong>0.80 (80%)</strong> best validation accuracy on the same data.
          </li>
          <li>
            <strong>Best ResNet18 (pretrained):</strong> a ResNet18 model with learning rate 0.0001 and
            augmentation achieved <strong>1.00 (100%)</strong> validation accuracy and very low validation loss.
          </li>
        </ul>
        <p>
          These results show that transfer learning and a stronger architecture like ResNet18 can make a big
          difference on a small, real-world style dataset.
        </p>
      </div>

      <div class="card">
        <h3>Batch Size &amp; Learning Rate Experiments</h3>
        <p>
          To understand how training hyperparameters affect performance, I varied both batch size and learning rate:
        </p>
        <ul>
          <li>
            <strong>Batch size:</strong> I trained AlexNet with batch sizes
            <strong>16, 32, and 64</strong>. Smaller batches (bs = 16) gave slightly higher best validation
            accuracy (≈ 98.3%) on this dataset, while larger batches (bs = 64) still performed very well
            (≈ 97–96%).
          </li>
          <li>
            <strong>Learning rate:</strong> With a conservative learning rate of <code>0.0001</code>, AlexNet
            converged smoothly and reached high validation accuracy. When I increased the learning rate to
            <code>0.001</code>, the model became unstable and validation accuracy stayed near 60%, illustrating
            how an overly large learning rate can hurt training.
          </li>
        </ul>
        <p>
          W&amp;B plots of training vs. validation accuracy and loss clearly show the difference between the
          stable 0.0001 runs and the noisy 0.001 run.
        </p>
      </div>
    </div>

    <div class="card-grid two-col">
      <div class="card">
        <h3>Data Augmentation vs. No Augmentation</h3>
        <p>
          I also compared runs with and without data augmentation. Augmented runs used random horizontal flips and
          standard ImageNet normalization; non-augmented runs only resized and normalized the images.
        </p>
        <ul>
          <li>
            <strong>With augmentation:</strong> multiple AlexNet runs with augmentation achieved high validation
            accuracy (up to ≈ 98.3%), and the learning curves showed smoother generalization to the validation set.
          </li>
          <li>
            <strong>Without augmentation:</strong> a comparable AlexNet run without augmentation still reached
            strong validation accuracy (≈ 96–97%), but augmentation is expected to help robustness, especially
            as the dataset grows or includes more varied conditions.
          </li>
        </ul>
        <p>
          For a safety-critical application like wildlife detection while driving, augmentation helps the model
          handle small viewpoint and lighting changes that drivers experience in the real world.
        </p>
      </div>

      <div class="card">
        <h3>AlexNet vs. ResNet18</h3>
        <p>
          Finally, I compared AlexNet to a deeper architecture, <strong>ResNet18</strong>, using the same dataset
          and a similar training setup (batch size 32, learning rate 0.0001, augmentation on).
        </p>
        <ul>
          <li>
            <strong>AlexNet (pretrained):</strong> best validation accuracy around <strong>98%</strong>.
          </li>
          <li>
            <strong>ResNet18 (pretrained):</strong> reached <strong>100%</strong> validation accuracy with very
            low validation loss (≈ 0.003), and both training and validation accuracy quickly climbed to 1.0.
          </li>
        </ul>
        <p>
          This comparison suggests that modern residual networks like ResNet18 can learn even more effective
          features for Safesight’s deer vs nodeer classification task, especially when combined with transfer
          learning from ImageNet.
        </p>
      </div>
    </div>
  </div>
</section>

    <section class="section section-alt">
  <div class="container">
    <h2>Experiment Results & Visualizations</h2>

    <div class="card-grid two-col">
      <div class="card">
        <h3>AlexNet vs ResNet18 – Accuracy</h3>
        <img
          src="assets/images/resnet_train_valid_accuracy.png"
          alt="ResNet18 training and validation accuracy over epochs"
          class="img-responsive"
        />
        <p class="img-caption">
          Training and validation accuracy for pretrained ResNet18 compared to AlexNet.
          ResNet18 reaches <strong>100% validation accuracy</strong> and stays there with
          very low variance, while AlexNet typically peaks in the <strong>97–98%</strong>
          range. This suggests that the deeper residual architecture can extract slightly
          stronger features for this wildlife detection task.
        </p>
      </div>

      <div class="card">
        <h3>AlexNet vs ResNet18 – Loss</h3>
        <img
          src="assets/images/resnet_train_valid_loss.png"
          alt="ResNet18 training and validation loss over epochs"
          class="img-responsive"
        />
        <p class="img-caption">
          Validation loss for ResNet18 drops extremely low (close to zero) and stays stable
          across epochs, indicating very confident and consistent predictions on the validation
          set. AlexNet also achieves low loss, but ResNet18 converges faster and to a smaller
          final loss, aligning with its perfect validation accuracy.
        </p>
      </div>
    </div>

    <div class="card-grid two-col">
      <div class="card">
        <h3>AlexNet Confusion Matrix</h3>
        <img
          src="assets/images/confusion_matrix_alexnet.png"
          alt="Confusion matrix for AlexNet on validation data"
          class="img-responsive"
        />
        <p class="img-caption">
          Confusion matrix for the best pretrained AlexNet run. Most examples are correctly
          classified as <strong>deer</strong> or <strong>nodeer</strong>, with only a small
          number of misclassifications. This shows that AlexNet learns a strong decision
          boundary, but still makes occasional mistakes on challenging edge cases.
        </p>
      </div>

      <div class="card">
        <h3>ResNet18 Confusion Matrix</h3>
        <img
          src="assets/images/confusion_matrix_resnet.png"
          alt="Confusion matrix for ResNet18 on validation data"
          class="img-responsive"
        />
        <p class="img-caption">
          Confusion matrix for pretrained ResNet18. All validation images fall on the
          diagonal, meaning every <strong>deer</strong> and <strong>nodeer</strong> image
          was classified correctly (100% validation accuracy). On this dataset, ResNet18
          fully separates the two classes.
        </p>
      </div>
    </div>
  </div>
</section>

    <section class="section section-alt">
      <div class="container">
        <h2>Takeaways & Future Work</h2>
        <div class="card-grid two-col">
          <div class="card">
            <h3>Key Takeaways</h3>
            <ul>
              <li>
                Transfer learning with AlexNet provides a strong performance boost on a small, custom deer vs
                nodeer dataset.
              </li>
              <li>
                Even with limited data, careful organization, basic augmentation, and good experiment tracking
                can produce a reliable model.
              </li>
              <li>
                Tools like W&amp;B make it easy to compare many runs and avoid guessing which model is “best”.
              </li>
            </ul>
          </div>

          <div class="card">
            <h3>Future Work</h3>
            <ul>
              <li>Collect more real dashcam footage in diverse weather and lighting conditions.</li>
              <li>
                Extend the label space from binary deer/nodeer to multiple animal categories and “no animal”.
              </li>
              <li>
                Explore smaller, faster architectures for real-time deployment in vehicles or mobile devices.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>© Safesight – AlexNet Wildlife Detection Project</p>
    </div>
  </footer>
</body>
</html>

